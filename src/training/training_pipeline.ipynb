{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41668032",
   "metadata": {},
   "source": [
    "# Training pipeline\n",
    "1. **first stage**: train yolov11n on original dataset with 5 fold cross validation\n",
    "2. **first stage**: train yolov11n/s/m on subsampled dataset with 5 fold cross validation\n",
    "3. **first stage**: train yolov11l/x on subsampled dataset only on first fold\n",
    "4. **first stage**: train rtdetr-l/x on subsampled dataset only on first fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c748f420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -U ultralytics torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4a2a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import os\n",
    "import yaml\n",
    "import json\n",
    "import datetime\n",
    "import psutil\n",
    "from ultralytics import YOLO, RTDETR\n",
    "from pathlib import Path\n",
    "\n",
    "print('Torch:', torch.__version__, '| CUDA:', torch.version.cuda, '| Device count:', torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print('GPU:', torch.cuda.get_device_name(0))\n",
    "print('RAM:', f'{psutil.virtual_memory().total/1e9:.1f} GB')\n",
    "\n",
    "# Enable CUDNN benchmarking for faster training\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd3ab76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration and Helper Functions\n",
    "\n",
    "class TrainingConfig:\n",
    "    \"\"\"Configuration class for training parameters\"\"\"\n",
    "    \n",
    "    # Dataset paths - using absolute paths\n",
    "    K_FOLD_CV_PATH = \"datasets/k_fold_cv\"\n",
    "    K_FOLD_CV_AUGMENTED_PATH = \"datasets/k_fold_cv_subsampled\"\n",
    "    \n",
    "    # Training parameters\n",
    "    EPOCHS = 100\n",
    "    PATIENCE = 20\n",
    "    IMGSZ = 320\n",
    "    BATCH_SIZE = 32\n",
    "    WORKERS = 8\n",
    "    \n",
    "    # Model configurations\n",
    "    YOLO_MODELS = {\n",
    "        'yolo11n': 'yolo11n.pt',\n",
    "        'yolo11s': 'yolo11s.pt', \n",
    "        'yolo11m': 'yolo11m.pt',\n",
    "        'yolo11l': 'yolo11l.pt',\n",
    "        'yolo11x': 'yolo11x.pt'\n",
    "    }\n",
    "    \n",
    "    RTDETR_MODELS = {\n",
    "        'rtdetr-l': 'rtdetr-l.pt',\n",
    "        'rtdetr-x': 'rtdetr-x.pt'\n",
    "    }\n",
    "\n",
    "def get_training_params(model_size):\n",
    "    \"\"\"Get training parameters based on model size\"\"\"\n",
    "    base_params = {\n",
    "        'epochs': TrainingConfig.EPOCHS,\n",
    "        'imgsz': TrainingConfig.IMGSZ,\n",
    "        'patience': TrainingConfig.PATIENCE,\n",
    "        'amp': True,\n",
    "        'optimizer': 'AdamW',\n",
    "        'save_period': 10,\n",
    "        'workers': TrainingConfig.WORKERS,\n",
    "        'cache': True,\n",
    "        'half': True,\n",
    "        'pretrained': True,\n",
    "        'lr0': 1e-4,\n",
    "        'lrf': 0.01,\n",
    "        'momentum': 0.937,\n",
    "        'weight_decay': 1e-4,\n",
    "        'augment': False\n",
    "    }\n",
    "    \n",
    "    # Adjust batch size based on model size\n",
    "    if model_size in ['yolo11n', 'yolo11s']:\n",
    "        base_params['batch'] = 32\n",
    "    elif model_size in ['yolo11m', 'rtdetr-l']:\n",
    "        base_params['batch'] = 16\n",
    "    elif model_size in ['yolo11l', 'yolo11x', 'rtdetr-x']:\n",
    "        base_params['batch'] = 8\n",
    "    \n",
    "    return base_params\n",
    "\n",
    "def save_results(results, filename):\n",
    "    \"\"\"Save training results to JSON file\"\"\"\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filepath = f\"training_results_{filename}_{timestamp}.json\"\n",
    "    \n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(results, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"Results saved to: {filepath}\")\n",
    "    return filepath\n",
    "\n",
    "def train_model(model_type, model_name, data_yaml, fold_idx, experiment_name, project_dir=\"runs\"):\n",
    "    \"\"\"Train a single model on a specific fold\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Training {model_name} on Fold {fold_idx}\")\n",
    "    print(f\"Data: {data_yaml}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    try:\n",
    "        # Load model\n",
    "        if model_type == 'yolo':\n",
    "            model_path = TrainingConfig.YOLO_MODELS[model_name]\n",
    "            model = YOLO(model_path)\n",
    "        elif model_type == 'rtdetr':\n",
    "            model_path = TrainingConfig.RTDETR_MODELS[model_name]\n",
    "            model = RTDETR(model_path)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "        \n",
    "        # Get training parameters\n",
    "        train_params = get_training_params(model_name)\n",
    "        \n",
    "        # Set training-specific parameters\n",
    "        train_params.update({\n",
    "            'data': data_yaml,\n",
    "            'name': f\"{model_name}_fold_{fold_idx}\",\n",
    "            'project': f\"{project_dir}/{experiment_name}\"\n",
    "        })\n",
    "        \n",
    "        print(f\"Training parameters: {train_params}\")\n",
    "        \n",
    "        # Train the model\n",
    "        results = model.train(**train_params)\n",
    "        \n",
    "        # Extract metrics\n",
    "        fold_result = {\n",
    "            'model_type': model_type,\n",
    "            'model_name': model_name,\n",
    "            'fold': fold_idx,\n",
    "            'status': 'completed',\n",
    "            'best_epoch': results.best_epoch if hasattr(results, 'best_epoch') else None,\n",
    "            'best_fitness': results.best_fitness if hasattr(results, 'best_fitness') else None,\n",
    "            'model_path': model.ckpt_path if hasattr(model, 'ckpt_path') else None,\n",
    "            'experiment_name': experiment_name,\n",
    "            'data_yaml': data_yaml,\n",
    "            'training_params': train_params\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n‚úÖ {model_name} Fold {fold_idx} completed successfully!\")\n",
    "        print(f\"Best epoch: {fold_result['best_epoch']}\")\n",
    "        print(f\"Best fitness: {fold_result['best_fitness']:.4f}\" if fold_result['best_fitness'] else \"Best fitness: N/A\")\n",
    "        print(f\"Model saved: {fold_result['model_path']}\")\n",
    "        \n",
    "        return fold_result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error training {model_name} on fold {fold_idx}: {str(e)}\")\n",
    "        return {\n",
    "            'model_type': model_type,\n",
    "            'model_name': model_name,\n",
    "            'fold': fold_idx,\n",
    "            'status': 'failed',\n",
    "            'error': str(e),\n",
    "            'experiment_name': experiment_name,\n",
    "            'data_yaml': data_yaml\n",
    "        }\n",
    "\n",
    "print(\"Configuration and helper functions loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13d56ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Dataset Structure\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"DATASET VERIFICATION\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "def verify_dataset_structure(dataset_path, dataset_name):\n",
    "    \"\"\"Verify the structure of a k-fold dataset\"\"\"\n",
    "    print(f\"\\nüìÅ Verifying {dataset_name}: {dataset_path}\")\n",
    "    \n",
    "    if not os.path.exists(dataset_path):\n",
    "        print(f\"‚ùå Dataset path not found: {dataset_path}\")\n",
    "        return False\n",
    "    \n",
    "    all_folds_valid = True\n",
    "    \n",
    "    for fold_idx in range(5):\n",
    "        fold_dir = os.path.join(dataset_path, f\"fold_{fold_idx}\")\n",
    "        data_yaml = os.path.join(fold_dir, \"data.yaml\")\n",
    "        \n",
    "        if os.path.exists(data_yaml):\n",
    "            # Count images in train and val\n",
    "            train_images = os.path.join(fold_dir, \"train\", \"images\")\n",
    "            val_images = os.path.join(fold_dir, \"val\", \"images\")\n",
    "            \n",
    "            train_count = len([f for f in os.listdir(train_images) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]) if os.path.exists(train_images) else 0\n",
    "            val_count = len([f for f in os.listdir(val_images) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]) if os.path.exists(val_images) else 0\n",
    "            \n",
    "            total = train_count + val_count\n",
    "            \n",
    "            if total > 0:\n",
    "                print(f\"  ‚úÖ Fold {fold_idx}: Train={train_count}, Val={val_count}, Total={total}\")\n",
    "            else:\n",
    "                print(f\"  ‚ùå Fold {fold_idx}: No images found\")\n",
    "                all_folds_valid = False\n",
    "        else:\n",
    "            print(f\"  ‚ùå Fold {fold_idx}: data.yaml not found\")\n",
    "            all_folds_valid = False\n",
    "    \n",
    "    return all_folds_valid\n",
    "\n",
    "# Verify both datasets\n",
    "k_fold_cv_valid = verify_dataset_structure(TrainingConfig.K_FOLD_CV_PATH, \"k_fold_cv\")\n",
    "k_fold_cv_augmented_valid = verify_dataset_structure(TrainingConfig.K_FOLD_CV_AUGMENTED_PATH, \"k_fold_cv_augmented\")\n",
    "\n",
    "print(f\"\\nüìä VERIFICATION SUMMARY:\")\n",
    "print(f\"k_fold_cv: {'‚úÖ Valid' if k_fold_cv_valid else '‚ùå Issues found'}\")\n",
    "print(f\"k_fold_cv_augmented: {'‚úÖ Valid' if k_fold_cv_augmented_valid else '‚ùå Issues found'}\")\n",
    "\n",
    "if k_fold_cv_valid and k_fold_cv_augmented_valid:\n",
    "    print(f\"\\nüöÄ All datasets verified! Ready to start training pipeline.\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Please fix dataset issues before proceeding with training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c383ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 1: Train YOLO11n on all folds of k_fold_cv\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"STAGE 1: Training YOLO11n on all folds of k_fold_cv\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "stage1_results = []\n",
    "k_fold_cv_path = TrainingConfig.K_FOLD_CV_PATH\n",
    "\n",
    "# Check if dataset exists\n",
    "if not os.path.exists(k_fold_cv_path):\n",
    "    print(f\"‚ùå Dataset path not found: {k_fold_cv_path}\")\n",
    "else:\n",
    "    print(f\"üìÅ Dataset path: {k_fold_cv_path}\")\n",
    "    \n",
    "    # Train on all 5 folds (0-4)\n",
    "    for fold_idx in range(5):\n",
    "        fold_dir = os.path.join(k_fold_cv_path, f\"fold_{fold_idx}\")\n",
    "        data_yaml = os.path.join(fold_dir, \"data.yaml\")\n",
    "        \n",
    "        if os.path.exists(data_yaml):\n",
    "            result = train_model(\n",
    "                model_type='yolo',\n",
    "                model_name='yolo11n',\n",
    "                data_yaml=data_yaml,\n",
    "                fold_idx=fold_idx,\n",
    "                experiment_name='stage1_yolo11n_k_fold_cv'\n",
    "            )\n",
    "            stage1_results.append(result)\n",
    "        else:\n",
    "            print(f\"‚ùå Data YAML not found: {data_yaml}\")\n",
    "            stage1_results.append({\n",
    "                'model_name': 'yolo11n',\n",
    "                'fold': fold_idx,\n",
    "                'status': 'failed',\n",
    "                'error': f'Data YAML not found: {data_yaml}'\n",
    "            })\n",
    "\n",
    "# Save Stage 1 results\n",
    "stage1_file = save_results(stage1_results, \"stage1\")\n",
    "\n",
    "# Summary\n",
    "successful_stage1 = [r for r in stage1_results if r['status'] == 'completed']\n",
    "print(f\"\\nüìä Stage 1 Summary: {len(successful_stage1)}/5 folds completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfeef21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 2: Train YOLO11n, s, and m on all folds of subsampled\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"STAGE 2: Training YOLO11n, s, and m on all folds of k_fold_cv_subsampled\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "stage2_results = []\n",
    "k_fold_cv_augmented_path = TrainingConfig.K_FOLD_CV_AUGMENTED_PATH\n",
    "stage2_models = ['yolo11m']\n",
    "\n",
    "# Check if dataset exists\n",
    "if not os.path.exists(k_fold_cv_augmented_path):\n",
    "    print(f\"‚ùå Dataset path not found: {k_fold_cv_augmented_path}\")\n",
    "else:\n",
    "    print(f\"üìÅ Dataset path: {k_fold_cv_augmented_path}\")\n",
    "    \n",
    "    # Train each model on all 5 folds (0-4)\n",
    "    for model_name in stage2_models:\n",
    "        print(f\"\\nüöÄ Starting {model_name} training on all folds...\")\n",
    "        \n",
    "        for fold_idx in range(5):\n",
    "            fold_dir = os.path.join(k_fold_cv_augmented_path, f\"fold_{fold_idx}\")\n",
    "            data_yaml = os.path.join(fold_dir, \"data.yaml\")\n",
    "            \n",
    "            if os.path.exists(data_yaml):\n",
    "                result = train_model(\n",
    "                    model_type='yolo',\n",
    "                    model_name=model_name,\n",
    "                    data_yaml=data_yaml,\n",
    "                    fold_idx=fold_idx,\n",
    "                    experiment_name=f'stage2_{model_name}_k_fold_cv_subsampled'\n",
    "                )\n",
    "                stage2_results.append(result)\n",
    "            else:\n",
    "                print(f\"‚ùå Data YAML not found: {data_yaml}\")\n",
    "                stage2_results.append({\n",
    "                    'model_name': model_name,\n",
    "                    'fold': fold_idx,\n",
    "                    'status': 'failed',\n",
    "                    'error': f'Data YAML not found: {data_yaml}'\n",
    "                })\n",
    "\n",
    "# Save Stage 2 results\n",
    "stage2_file = save_results(stage2_results, \"stage2\")\n",
    "\n",
    "# Summary\n",
    "successful_stage2 = [r for r in stage2_results if r['status'] == 'completed']\n",
    "total_expected = len(stage2_models) * 5  # 3 models √ó 5 folds = 15\n",
    "print(f\"\\nüìä Stage 2 Summary: {len(successful_stage2)}/{total_expected} trainings completed successfully\")\n",
    "\n",
    "# Per-model summary\n",
    "for model_name in stage2_models:\n",
    "    model_results = [r for r in stage2_results if r['model_name'] == model_name and r['status'] == 'completed']\n",
    "    print(f\"  {model_name}: {len(model_results)}/5 folds completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbf1c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 3: Train YOLO11l and x on first fold of k_fold_cv_augmented\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"STAGE 3: Training YOLO11l and x on first fold of k_fold_cv_subsampled\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "stage3_results = []\n",
    "stage3_models = ['yolo11l', 'yolo11x']\n",
    "fold_idx = 0  # First fold\n",
    "\n",
    "# Check if dataset exists\n",
    "if not os.path.exists(k_fold_cv_augmented_path):\n",
    "    print(f\"‚ùå Dataset path not found: {k_fold_cv_augmented_path}\")\n",
    "else:\n",
    "    fold_dir = os.path.join(k_fold_cv_augmented_path, f\"fold_{fold_idx}\")\n",
    "    data_yaml = os.path.join(fold_dir, \"data.yaml\")\n",
    "    \n",
    "    if os.path.exists(data_yaml):\n",
    "        print(f\"üìÅ Using fold {fold_idx}: {data_yaml}\")\n",
    "        \n",
    "        # Train each large model on fold 0\n",
    "        for model_name in stage3_models:\n",
    "            print(f\"\\nüöÄ Starting {model_name} training on fold {fold_idx}...\")\n",
    "            \n",
    "            result = train_model(\n",
    "                model_type='yolo',\n",
    "                model_name=model_name,\n",
    "                data_yaml=data_yaml,\n",
    "                fold_idx=fold_idx,\n",
    "                experiment_name=f'stage3_{model_name}_fold0_subsampled'\n",
    "            )\n",
    "            stage3_results.append(result)\n",
    "    else:\n",
    "        print(f\"‚ùå Data YAML not found: {data_yaml}\")\n",
    "        for model_name in stage3_models:\n",
    "            stage3_results.append({\n",
    "                'model_name': model_name,\n",
    "                'fold': fold_idx,\n",
    "                'status': 'failed',\n",
    "                'error': f'Data YAML not found: {data_yaml}'\n",
    "            })\n",
    "\n",
    "# Save Stage 3 results\n",
    "stage3_file = save_results(stage3_results, \"stage3\")\n",
    "\n",
    "# Summary\n",
    "successful_stage3 = [r for r in stage3_results if r['status'] == 'completed']\n",
    "print(f\"\\nüìä Stage 3 Summary: {len(successful_stage3)}/{len(stage3_models)} large YOLO models completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615019c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 4: Train RT-DETR-l and x on first fold of k_fold_cv_augmented\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"STAGE 4: Training RT-DETR-l and x on first fold of k_fold_cv_subsampled\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "stage4_results = []\n",
    "stage4_models = ['rtdetr-l', 'rtdetr-x']\n",
    "fold_idx = 0  # First fold\n",
    "\n",
    "# Check if dataset exists\n",
    "if not os.path.exists(k_fold_cv_augmented_path):\n",
    "    print(f\"‚ùå Dataset path not found: {k_fold_cv_augmented_path}\")\n",
    "else:\n",
    "    fold_dir = os.path.join(k_fold_cv_augmented_path, f\"fold_{fold_idx}\")\n",
    "    data_yaml = os.path.join(fold_dir, \"data.yaml\")\n",
    "    \n",
    "    if os.path.exists(data_yaml):\n",
    "        print(f\"üìÅ Using fold {fold_idx}: {data_yaml}\")\n",
    "        \n",
    "        # Train each RT-DETR model on fold 0\n",
    "        for model_name in stage4_models:\n",
    "            print(f\"\\nüöÄ Starting {model_name} training on fold {fold_idx}...\")\n",
    "            \n",
    "            result = train_model(\n",
    "                model_type='rtdetr',\n",
    "                model_name=model_name,\n",
    "                data_yaml=data_yaml,\n",
    "                fold_idx=fold_idx,\n",
    "                experiment_name=f'stage4_{model_name}_fold0_subsampled'\n",
    "            )\n",
    "            stage4_results.append(result)\n",
    "    else:\n",
    "        print(f\"‚ùå Data YAML not found: {data_yaml}\")\n",
    "        for model_name in stage4_models:\n",
    "            stage4_results.append({\n",
    "                'model_name': model_name,\n",
    "                'fold': fold_idx,\n",
    "                'status': 'failed',\n",
    "                'error': f'Data YAML not found: {data_yaml}'\n",
    "            })\n",
    "\n",
    "# Save Stage 4 results\n",
    "stage4_file = save_results(stage4_results, \"stage4\")\n",
    "\n",
    "# Summary\n",
    "successful_stage4 = [r for r in stage4_results if r['status'] == 'completed']\n",
    "print(f\"\\nüìä Stage 4 Summary: {len(successful_stage4)}/{len(stage4_models)} RT-DETR models completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aeeb339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Summary and Analysis\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"COMPREHENSIVE TRAINING PIPELINE - FINAL SUMMARY\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Combine all results\n",
    "all_results = stage1_results + stage2_results + stage3_results + stage4_results\n",
    "\n",
    "# Overall statistics\n",
    "total_trainings = len(all_results)\n",
    "successful_trainings = len([r for r in all_results if r['status'] == 'completed'])\n",
    "failed_trainings = total_trainings - successful_trainings\n",
    "\n",
    "print(f\"\\nüìä OVERALL STATISTICS:\")\n",
    "print(f\"Total trainings attempted: {total_trainings}\")\n",
    "print(f\"Successful trainings: {successful_trainings}\")\n",
    "print(f\"Failed trainings: {failed_trainings}\")\n",
    "print(f\"Success rate: {(successful_trainings/total_trainings)*100:.1f}%\")\n",
    "\n",
    "# Stage-by-stage breakdown\n",
    "print(f\"\\nüìã STAGE BREAKDOWN:\")\n",
    "stages = [\n",
    "    (\"Stage 1 (YOLO11n on k_fold_cv)\", stage1_results, 5),\n",
    "    (\"Stage 2 (YOLO11n,s,m on k_fold_cv_augmented)\", stage2_results, 15),\n",
    "    (\"Stage 3 (YOLO11l,x on fold 0)\", stage3_results, 2),\n",
    "    (\"Stage 4 (RT-DETR-l,x on fold 0)\", stage4_results, 2)\n",
    "]\n",
    "\n",
    "for stage_name, stage_results, expected_count in stages:\n",
    "    successful = len([r for r in stage_results if r['status'] == 'completed'])\n",
    "    print(f\"  {stage_name}: {successful}/{expected_count} completed\")\n",
    "\n",
    "# Model performance summary (for successful trainings)\n",
    "print(f\"\\nüèÜ MODEL PERFORMANCE SUMMARY:\")\n",
    "successful_results = [r for r in all_results if r['status'] == 'completed' and r.get('best_fitness')]\n",
    "\n",
    "if successful_results:\n",
    "    # Group by model\n",
    "    model_performance = {}\n",
    "    for result in successful_results:\n",
    "        model_name = result['model_name']\n",
    "        if model_name not in model_performance:\n",
    "            model_performance[model_name] = []\n",
    "        model_performance[model_name].append(result['best_fitness'])\n",
    "    \n",
    "    # Calculate averages\n",
    "    for model_name, fitness_values in model_performance.items():\n",
    "        avg_fitness = sum(fitness_values) / len(fitness_values)\n",
    "        max_fitness = max(fitness_values)\n",
    "        min_fitness = min(fitness_values)\n",
    "        count = len(fitness_values)\n",
    "        \n",
    "        print(f\"  {model_name}: Avg={avg_fitness:.4f}, Max={max_fitness:.4f}, Min={min_fitness:.4f} ({count} runs)\")\n",
    "\n",
    "# Best performing models\n",
    "if successful_results:\n",
    "    best_model = max(successful_results, key=lambda x: x['best_fitness'])\n",
    "    print(f\"\\nü•á BEST PERFORMING MODEL:\")\n",
    "    print(f\"  Model: {best_model['model_name']}\")\n",
    "    print(f\"  Fold: {best_model['fold']}\")\n",
    "    print(f\"  Fitness: {best_model['best_fitness']:.4f}\")\n",
    "    print(f\"  Path: {best_model.get('model_path', 'N/A')}\")\n",
    "\n",
    "# Failed trainings summary\n",
    "failed_results = [r for r in all_results if r['status'] == 'failed']\n",
    "if failed_results:\n",
    "    print(f\"\\n‚ùå FAILED TRAININGS:\")\n",
    "    for result in failed_results:\n",
    "        print(f\"  {result['model_name']} (Fold {result['fold']}): {result.get('error', 'Unknown error')}\")\n",
    "\n",
    "# Save comprehensive results\n",
    "comprehensive_results = {\n",
    "    'timestamp': datetime.datetime.now().isoformat(),\n",
    "    'summary': {\n",
    "        'total_trainings': total_trainings,\n",
    "        'successful_trainings': successful_trainings,\n",
    "        'failed_trainings': failed_trainings,\n",
    "        'success_rate': (successful_trainings/total_trainings)*100\n",
    "    },\n",
    "    'stages': {\n",
    "        'stage1': stage1_results,\n",
    "        'stage2': stage2_results,\n",
    "        'stage3': stage3_results,\n",
    "        'stage4': stage4_results\n",
    "    },\n",
    "    'all_results': all_results\n",
    "}\n",
    "\n",
    "comprehensive_file = save_results(comprehensive_results, \"comprehensive_pipeline\")\n",
    "\n",
    "print(f\"\\n‚úÖ Comprehensive training pipeline completed!\")\n",
    "print(f\"üìÅ All results saved to: {comprehensive_file}\")\n",
    "print(f\"\\nüîç Check the 'runs/' directory for individual training outputs and models.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
